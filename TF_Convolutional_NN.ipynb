{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "matplotlib.rcParams['savefig.dpi'] = 144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python2.7/site-packages/IPython/html.py:14: ShimWarning: The `IPython.html` package has been deprecated since IPython 4.0. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.\n",
      "  \"`IPython.html.widgets` has moved to `ipywidgets`.\", ShimWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import pylib.conv_widget as cw\n",
    "from pylib.tensorboardcmd import tensorboard_cmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = None\n",
    "\n",
    "def reset_vars():\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "def reset_tf():\n",
    "    global sess\n",
    "    if sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()\n",
    "    sess = tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- requirement: pylib/__init__.py -->\n",
    "<!-- requirement: pylib/conv_widget.py -->\n",
    "<!-- requirement: images/conv_movie.gif -->\n",
    "<!-- requirement: images/portal-v.png -->\n",
    "<!-- requirement: images/portal-h.png -->\n",
    "<!-- requirement: images/conv_weights.png -->\n",
    "<!-- requirement: images/conv_layer.png -->\n",
    "<!-- requirement: images/CNN_schematic.png -->\n",
    "\n",
    "# Convolutional Neural Networks\n",
    "\n",
    "In the previous exercise, we flattened the MNIST images into vectors before feeding them into our model. Doing so, however, destroys information about shapes that are present in clusters of neighboring pixels. We can preserve this two dimensional information by using Convolutional Neural Networks (CNNs). \n",
    "\n",
    "CNNs are inspired by the [visual cortex](https://en.wikipedia.org/wiki/Visual_cortex), which is responsible for image processing in animals. Neurons in the visual cortex fire in response to stimuli that activate small sub-regions of an animal's visual field. We can therefore think of these neurons as filters that detect patterns across the animal's visual field. \n",
    "\n",
    "We will train a CNN to find a set of filters that can detect repeated shapes in MNIST images. Convolving the image with a filter produces another image with highlighted features called a feature map. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutions\n",
    "\n",
    "A convolution is a mathematical operation that is performed on two functions. For continuous functions, it is defined as:\n",
    "\n",
    "$$ (f * g)(t) = \\int_{-\\infty}^{\\infty} f(\\tau)g(t-\\tau) d\\tau $$\n",
    "\n",
    "For discrete functions, it is:\n",
    "\n",
    "$$ (f * g)(t) = \\sum_{m=-\\infty}^{\\infty} f[m] g[n-m] $$\n",
    "\n",
    "A function that is a convolution of two other functions can be thought of as the area between the two original functions as one function is translated. \n",
    "\n",
    "![convolution](images/conv_movie.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolving images and filters\n",
    "\n",
    "Play around with the widget below, which demonstrates the process of convolving a 7x7 pixel image with a 3x3 pixel filter. At each iteration, the value of one pixel in the image (denoted by the red square) is replaced by the average of the pixel values within the 3x3 filter. In convolutional neural networks, a weight is assigned to each pixel in the filter, and consequently, the \"red pixel\" is replaced by weighted sum of the surrounded pixels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkkAAAEsCAYAAADXUSdAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0d26432a10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image = np.zeros((7, 7), dtype=float)\n",
    "for i in xrange(1, 6):\n",
    "    for j in xrange(6-i, 6):\n",
    "        image[i][j] = 1\n",
    "\n",
    "img_filter = np.ones((3, 3), dtype=float)\n",
    "img_filter /= img_filter.size\n",
    "\n",
    "titles = ('Image and kernel', 'Filtered image')\n",
    "convwidget = cw.ConvWidget()\n",
    "convwidget.interactive_convolution_demo(image, img_filter, vmax=1, titles=titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Strides\n",
    "\n",
    "In the interactive widget, we perform the convolution at each pixel. However, we can choose to skip pixels. In TensorFlow, we can set the number of pixels our filter moves at each step, or the **stride**. How does the stride effect the dimension of the filtered image? Why would we want to change the stride?\n",
    "\n",
    "In TensorFlow's `conv2d` function, we set the stride with `strides=[batch, stride_width, stride_height, channels]`. For the MNIST dataset, `strides=[1, k, k, 1]`. Since we are dealing with grayscale images, the number of channels is 1. If we had colored images, this value would be set to 3 (corresponding red, green, and blue values). \n",
    "\n",
    "#### Padding\n",
    "\n",
    "Notice how the filter extends past the edge of the image while performing convolutions on some pixels. To deal with these edge cases, we typically **pad** the image or add more pixels to the perimeter of the image. This process ensures that the dimensions of the input image are the same as the dimensions of the output image. In the case of the MNIST dataset, the image is padded with zeros (or white space). When might we run into problems when padding images?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Applying a simple edge filter on a handwritten digit\n",
    "\n",
    "The magic behind the convolutional neural network lies in the fact that each filter learns to recognize certain patterns repeated throughout images. To build our intuition for why this is the case, we will consider two very simple filters $(1,-1)$ and $(1,-1)^T$ applied to an image consisting of only black and white pixels (with values of 1 or 0). The image consists of a black, horizontal line connected to a black, vertical line and roughly resembles a \"7.\" See the cell below. We see from the two resulting convolved images that the filters picked out vertical and horizontal edges, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0  0  0]\n",
      " [ 1  0  0  0 -1]\n",
      " [ 0  0  0  1 -1]\n",
      " [ 0  0  0  1 -1]\n",
      " [ 0  0  0  1 -1]]\n",
      "[[ 0  0  0  0  0]\n",
      " [ 1  1  1  1  0]\n",
      " [-1 -1 -1  0  0]\n",
      " [ 0  0  0  0  0]\n",
      " [ 0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "from scipy import signal as sg\n",
    "ex_image = [[0, 0, 0, 0, 0],\n",
    "            [1, 1, 1, 1, 0], # <---horizontal line of ones\n",
    "            [0, 0, 0, 1, 0],\n",
    "            [0, 0, 0, 1, 0],\n",
    "            [0, 0, 0, 1, 0]]\n",
    "                    # ^ vertical line of ones\n",
    "\n",
    "print(sg.convolve(ex_image, [[1, -1]], \"same\")) #vertical edge\n",
    "print(sg.convolve(ex_image, [[1], [-1]], \"same\")) #horizontal edge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we apply the vertical and horizontal edge filters a handwritten 7, we get the following feature maps:\n",
    "<table>\n",
    "    <tr>\n",
    "        <td> <img src=\"files/images/portal-v.png\" style=\"width: 250px;\"/> </td>\n",
    "        <td> <img src=\"files/images/portal-h.png\" style=\"width: 250px;\"/> </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional neural networks\n",
    "\n",
    "Convolutional neural networks are comprised of a series of convolutional and max pooling layers, followed by a fully connected layer. Each of these will be explained in detail below. Let's first start off by repeating some of the steps we did in the previous notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "reset_tf()\n",
    "\n",
    "# Load data\n",
    "data = input_data.read_data_sets('/tmp/data/', one_hot=True)\n",
    "\n",
    "# Get class (number) for test data\n",
    "data.test.cls = np.argmax(data.test.labels, axis=1)\n",
    "\n",
    "# Set path to summary logs\n",
    "now = datetime.now()\n",
    "logs_path = now.strftime(\"%Y%m%d-%H%M%S\") + '/summaries'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's set our model parameters and define our placeholder variables. This step is very similar to the one we did in the previous notebook, but now we assign the number of channels to `n_channels` and the filter size to `filt_size`. For the sake of simplicity, we will not use dropout in this model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "img_size = 28\n",
    "img_size_flat = img_size * img_size\n",
    "img_shape = (img_size, img_size)\n",
    "\n",
    "n_classes = 10\n",
    "n_channels = 1\n",
    "filt_size = [5, 5] # 5x5 pixel filters\n",
    "\n",
    "batch_size = 50\n",
    "num_iterations = 400 #1500\n",
    "display_step = 100\n",
    "\n",
    "# Placeholder variables\n",
    "x = tf.placeholder(tf.float32, shape=[None, img_size_flat], name='x')\n",
    "y_true = tf.placeholder(tf.float32, shape=[None, n_classes], name='y_true')\n",
    "y_true_cls = tf.argmax(y_true, dimension=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional layers\n",
    "\n",
    "The purpose of the convolutional layer is to extract features from images. In this layer, a set of filters are convolved with some input to produce activation or feature maps. Then, the ReLU operation is applied to each pixel in the feature maps, converting negative pixel values to zero. During the training process, the CNN learns which filters activate when a certain features are present. \n",
    "\n",
    "In the example below, we choose 16 filters and a stride of 1. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_reshape = tf.reshape(x, shape=[-1, img_size, img_size, 1])\n",
    "out_conv = tf.layers.conv2d(x_reshape, 16, filt_size, padding='same', activation=tf.nn.relu, name=\"convolution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will add the kernels to the TensorBoard output, so that we can visualize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'kernel_weights:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernels = tf.get_default_graph().get_operation_by_name('convolution/kernel').values()[0]\n",
    "shape = kernels.shape.as_list()  #[dx, dy, ch, n_filters]\n",
    "w = tf.reshape(kernels, [shape[3], shape[0], shape[1], shape[2]])\n",
    "tf.summary.image('kernel_weights', w, max_outputs=shape[3], collections=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, our 16 filters will look similar to those shown in the figure on the left. Red represents positive values and blue represents negative values. If we convolve an image of a seven with these filters, we will get the feature maps in the figure on the right.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td> <img src=\"files/images/conv_weights.png\" style=\"width: 450px;\"/> </td>\n",
    "        <td> <img src=\"files/images/conv_layer.png\" style=\"width: 450px;\"/> </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max pooling \n",
    "In image classification, the approximate locations of shapes and edges are more important than their exact locations. To add this type of flexibility into our model, we can downsample our convolved images. A variety of techniques for downsampling exist, but **max pooling** is a popular one. Max pooling partitions an image and outputs the maximum value of each partition. A commonly used partition or filter size is 2x2. In TensorFlow, we set the stride of this filter to its dimension, such that pooling regions do not overlap with one another. \n",
    "\n",
    "What happens to the resolution of an nxn image after this type of pooling?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_pool = tf.layers.max_pooling2d(out_conv, pool_size=(2, 2), strides=(2,2), padding='same')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully connected layer\n",
    "\n",
    "Notice that a convolution-ReLU-pooling layer operates on a single feature map (one for each convolutional filter in the previous layer), where each map represents a high-level feature in the input image. The goal of our convolutonal neural network is to (1) identify each feature in the image, (2) identify combinations of these features in the image, and then (3) use this information to classify the image. To this end, we eventually want to connect every neuron to one another in a **fully connected layer**. \n",
    "\n",
    "You can think of a fully connected layer as a layer in a basic or flat neural network. To create this layer in TensorFlow, we reshape our 3-dimensional output into a vector and then apply the weights and biases. The size of the weights matrix is the total number of pixels (7 $*$ 7 $*$ 16) by the number of desired outputs. In this example, we choose 100 as the output size. \n",
    "\n",
    "You can have more than one fully connected layer, but the final activation function should be softmax. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_pool_reshape = tf.reshape(out_pool, [-1, out_pool.shape[1:].num_elements()])\n",
    "out = tf.layers.dense(out_pool_reshape, 100, activation=tf.nn.relu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class prediction\n",
    "\n",
    "Finally we pass this output through another layer with output size `n_classes`. This step is necessary to get our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = tf.layers.dense(out, n_classes, activation=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Completing the model\n",
    "\n",
    "We then define our loss and accuracy as we did in our previous models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'accuracy:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loss\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_pred, labels=y_true))\n",
    "tf.summary.scalar('loss', loss)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "# Accuracy\n",
    "y_pred_cls = tf.argmax(y_pred, dimension=1)\n",
    "correct_prediction = tf.equal(y_pred_cls, y_true_cls)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "tf.summary.scalar('accuracy',accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will then initialize our variables, launch our graph, and train our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all summaries\n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "# Initialize\n",
    "reset_vars()\n",
    "\n",
    "# Create summary writers\n",
    "train_writer = tf.summary.FileWriter(logs_path + '/train', graph=tf.get_default_graph())\n",
    "test_writer = tf.summary.FileWriter(logs_path + '/test', graph=tf.get_default_graph())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization Iteration:      0, Test Loss: 2.24337220192, Test Accuracy:  18.1%\n",
      "Optimization Iteration:    100, Test Loss: 0.324235439301, Test Accuracy:  90.9%\n",
      "Optimization Iteration:    200, Test Loss: 0.213373363018, Test Accuracy:  93.8%\n",
      "Optimization Iteration:    300, Test Loss: 0.169864967465, Test Accuracy:  95.3%\n",
      "Optimization Iteration:    399, Test Loss: 0.140508472919, Test Accuracy:  95.7%\n",
      "Time usage: 0:00:31\n"
     ]
    }
   ],
   "source": [
    "def optimize(num_iterations):\n",
    "    # Start-time used for printing time-usage below.\n",
    "    start_time = time.time()\n",
    "\n",
    "    step = 1\n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        # Get a batch of training examples.\n",
    "        x_batch, y_true_batch = data.train.next_batch(batch_size)\n",
    "\n",
    "        # ---------------------- TRAIN -------------------------\n",
    "        # Optimize model\n",
    "        sess.run(optimizer, feed_dict={x: x_batch,y_true: y_true_batch}) \n",
    "        \n",
    "        \n",
    "        # Print status every 100 iterations.\n",
    "        if (i % display_step == 0) or (i == num_iterations - 1):\n",
    "            \n",
    "            summary = sess.run(merged, feed_dict={x: x_batch, y_true: y_true_batch})\n",
    "            train_writer.add_summary(summary, step)\n",
    "            \n",
    "            #----------------------- TEST ---------------------------\n",
    "            # Test model\n",
    "            summary, l, acc = sess.run([merged, loss, accuracy], feed_dict={x: data.test.images,\n",
    "                                                                            y_true: data.test.labels})                                                          \n",
    "            test_writer.add_summary(summary, step)\n",
    "            \n",
    "            # Message for network evaluation\n",
    "            msg = \"Optimization Iteration: {0:>6}, Test Loss: {1:>6}, Test Accuracy: {2:>6.1%}\"\n",
    "            print(msg.format(i, l, acc))\n",
    "            \n",
    "            step += 1\n",
    "\n",
    "    # Ending time.\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Difference between start and end-times.\n",
    "    time_dif = end_time - start_time\n",
    "\n",
    "    # Print the time-usage.\n",
    "    print(\"Time usage: \" + str(timedelta(seconds=int(round(time_dif)))))\n",
    "    \n",
    "optimize(num_iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will close our summary writers. We can also view our graph on TensorBoard. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>Run at the command line:\n",
       "    <tt>tensorboard --logdir=20170926-150314/summaries</tt><br />\n",
       "    Then open <a href=\"http://10.60.3.5:6006/\" target=\"_blank\">http://10.60.3.5:6006/</a></p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Close summary writers\n",
    "train_writer.close()\n",
    "test_writer.close()\n",
    "\n",
    "tensorboard_cmd(logs_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use our model to make a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: 7, Actual: 7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f0d263f2dd0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUsAAAFKCAYAAACU6307AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nLnLBAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0d2424e6d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prediction = tf.argmax(y_pred, 1)\n",
    "\n",
    "def predict(idx):\n",
    "    image = data.test.images[idx]\n",
    "    return sess.run(prediction, feed_dict={x: [image]})\n",
    "\n",
    "idx = 0\n",
    "actual = np.argmax(data.test.labels[idx])\n",
    "print (\"Predicted: %d, Actual: %d\" % (predict(idx), actual))\n",
    "plt.imshow(data.test.images[idx].reshape((28,28)), cmap=plt.cm.gray_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We go through the following steps when training a CNN:\n",
    "1. Initialize the filters/weights and biases with random values.\n",
    "2. Perform a series of convolution, ReLU, and pooling operations to produce feature maps.\n",
    "3. Flatten the feature maps and apply weights and biases\n",
    "4. Pass the output from the fully connected layer through a final, softmax layer\n",
    "5. Calculate loss.\n",
    "6. Minimize loss via gradient descent and adjust values of filters/weights and biases. \n",
    "7. Repeat previous steps with all training images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Improving the network architecture\n",
    "\n",
    "We can improve the model's accuracy by adding more layers and filters. Rewrite the model so that its architecture matches that in the figure below. Bonus points if you use dropout. \n",
    "\n",
    "![CNN schematic](images/CNN_schematic.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright &copy; 2017 The Data Incubator.  All rights reserved.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "widgets": {
   "state": {
    "757efa5b5613458189995bbbe076e124": {
     "views": [
      {
       "cell_index": 5
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
